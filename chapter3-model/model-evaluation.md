---
description: Model Errorï¼ŒBias-Variance tradeoffï¼ŒRegularization
---

# Regularization and Feature Selection

æœºå™¨å­¦ä¹ çš„æœ¬è´¨ä¸æ˜¯â€œGiven x, y=\*\*â€œï¼Œè€Œæ˜¯â€œGiven x, a probability distribution â€œã€‚å¾—åˆ°çš„æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œä¹Ÿå°±æ˜¯modelçš„é‚£äº›assumptionï¼Œè§£å‡ºæ¥çš„æ˜¯é‚£äº›betaç³»æ•°ã€‚è¿™å°±æœ‰äº†æ‰€è°“çš„confidence intervalçš„è¯´æ³•ã€‚

æœ€åæ˜¯å†³ç­–äººè‡ªå·±æ ¹æ®yçš„distributionï¼Œå¾—åˆ°yçš„å–å€¼ã€‚

![](https://cdn.mathpix.com/snip/images/jDR8vTD7Qf9MwcKo_u7kpbh_NxIBQ6uTHhenaJcPXW4.original.fullsize.png)



## Model Error

$$
\text {Error}=\text {bias}^{2}+\text {variance}+\text {Irreducible error}
$$

ä¸Šé¢çš„å…¬å¼ä¸æ˜¯ä¸€ä¸ªâ€œè®¡ç®—biasæˆ–varianceâ€œçš„æ–¹æ³•ï¼Œåªæ˜¯ä¸€ä¸ªå¤§å®¶ç†è§£erroræ¥æºçš„æ–¹æ³•ã€‚æˆ‘ä»¬å–œæ¬¢ç”¨least squareæ¥æ„é€ loss functionï¼Œæ˜¯å› ä¸ºä½¿ç”¨least squareæ—¶irreducible erroræ¶ˆå¤±äº†ã€‚

### Mean Squared Error \(MSE\)

è¡¨ç°å½¢å¼ä¸Šçœ‹ç€æ˜¯least squareï¼ˆè¿™æ˜¯ä¸€ä¸ªfunctionï¼‰çš„å½¢å¼ï¼Œä½†åœ¨è¿™é‡Œè¿™æ˜¯ç”¨æ¥è¡¨ç°errorçš„æ–¹æ³•ã€‚å¦‚æœæˆ‘ä»¬æŠŠå…¬å¼ä¸­çš„thetaçœ‹ä½œyï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯least squareã€‚å¼•ç”¨MSEæ˜¯ä¸ºäº†è¡¡é‡modelçš„ï¼Œæ˜¯ä¸€ä¸ªç¡®è®¤å€¼ï¼Œå¯ä»¥ç†è§£ä¸º a static metric of random variableã€‚

$$
\begin{aligned} \operatorname{MSE}(\hat{\theta}) \equiv \mathbb{E}\left((\hat{\theta}-\theta)^{2}\right) &=\mathbb{E}\left[(\hat{\theta}-\mathbb{E}(\hat{\theta})+\mathbb{E}(\hat{\theta})-\theta)^{2}\right] \\ &=\mathbb{E}\left[(\hat{\theta}-\mathbb{E}(\hat{\theta}))^{2}+2((\hat{\theta}-\mathbb{E}(\hat{\theta}))(\mathbb{E}(\hat{\theta})-\theta))+(\mathbb{E}(\hat{\theta})-\theta)^{2}\right] \\ &=\mathbb{E}\left[(\hat{\theta}-\mathbb{E}(\hat{\theta}))^{2}\right]+2 \mathbb{E}[(\hat{\theta}-\mathbb{E}(\hat{\theta}))(\mathbb{E}(\hat{\theta})-\theta)]+\mathbb{E}\left[(\mathbb{E}(\hat{\theta})-\theta)^{2}\right] \\ &=\mathbb{E}\left[(\hat{\theta}-\mathbb{E}(\hat{\theta}))^{2}\right]+2(\mathbb{E}(\hat{\theta})-\theta) \overline{\mathbb{E}(\hat{\theta})-\mathbb{E}(\hat{\theta})}+\mathbb{E}\left[(\mathbb{E}(\hat{\theta})-\theta)^{2}\right] \\ &=\mathbb{E}\left[(\hat{\theta}-\mathbb{E}(\hat{\theta}))^{2}\right]+\mathbb{E}\left[(\mathbb{E}(\hat{\theta})-\theta)^{2}\right] \\ &=\operatorname{Var}(\hat{\theta})+\operatorname{Bias}(\hat{\theta}, \theta)^{2} \end{aligned}
$$

**Irreducible error:** æ— è®ºæ¢ä»€ä¹ˆmodelï¼Œè¿™ä¸ªerroréƒ½å­˜åœ¨ï¼›å®ƒä¸modelæ— å…³ï¼Œcannot be reduced

**æ¨¡å‹çš„å‡†ç¡®æ€§biasï¼š**è¿™ä¸ªmodelåœ¨ _è®­ç»ƒé›†ç¨æœ‰å˜åŒ–ä¸‹_ çš„å¹³å‡è¾“å‡ºç»“æœ**ä¸çœŸå®å€¼**ç›¸æ¯”çš„å¹³å‡å‡†ç¡®æ€§

**æ¨¡å‹çš„ç¨³å®šæ€§varianceï¼š**æŸä¸€æ¬¡modelçš„æ•°æ®ç»“æœä¸è¿™ä¸ªmodelçš„**å¹³å‡æ°´å¹³**çš„å·®è· çš„å¹³æ–¹çš„æœŸæœ›

æ‰“ä¸ªæ¯”æ–¹ï¼Œbiasè€ƒéªŒçš„æ˜¯å¹³å‡æ°´å¹³ï¼Œvarianceè€ƒéªŒçš„æ˜¯æœ¬æ¬¡çš„å‘æŒ¥æ°´å¹³ï¼ˆè¿æ°”ï¼‰



å¯¹äºBiasçš„ç†è§£ï¼šæ³¨æ„å¯¹äºbiasçš„è®¡ç®—ï¼Œå˜çš„æ˜¯modelè€Œä¸æ˜¯y=3x+5ä¸­çš„xã€‚å› ä¸ºæ‹¿ç€è¿™æ ·ä¸€ä¸ªequationï¼Œåªè¦x=3, å¾—åˆ°çš„yå°±æ˜¯å›ºå®šå€¼ï¼Œè¿™å°±æ²¡ä»€ä¹ˆbiaså¥½ç®—çš„ã€‚ æˆ‘ä»¬å…¶å®æ˜¯æ¯ä¸€æ¬¡ç¨å¾®æ”¹ä¸€ç‚¹è®­ç»ƒæ•°æ®åˆ›ä¸€ä¸ªä¸ªæœ‰ç»†å¾®å·®å¼‚çš„modelï¼Œç„¶åå†å›æ¥è¿˜æ˜¯åœ¨ç»™å®šçš„xä¸‹å»è®­ç»ƒyï¼Œè¿™ä¸€ç³»åˆ—çš„yç†è®ºä¸Šæ¥è¯´æ˜¯è¿‘ä¼¼çš„ï¼Œè¿™äº›yçš„å¹³å‡æ°´å¹³æ˜¯biasã€‚

å¯¹äºVarianceçš„ç†è§£ï¼šå¦‚æœæœ‰1000è¡Œæ•°æ®ï¼Œæ¢äº†1è¡Œå°±ç»“æœå¯¼è‡´æ¨¡å‹å®Œå…¨ä¸ä¸€æ ·äº†ï¼Œé‚£æ­¤æ—¶å°±æ˜¯high varianceï¼Œå®ƒæ²¡æœ‰å¾ˆå¥½çš„generalityã€‚æ‰€è°“çš„â€œover fittingâ€œå…¶å®ä¸æ˜¯erroræ›´é«˜äº†ï¼Œè€Œæ˜¯varianceé«˜ï¼Œå› ä¸ºbiasæ°¸è¿œæ˜¯éšç€modelçš„å¤æ‚åº¦å˜é«˜è€Œé™ä½çš„ï¼Œå¦‚ä¸‹å›¾ã€‚

![](https://cdn.mathpix.com/snip/images/SsK7WrhvodQpFBmbOBuPPhKR-x9vGvmj_rvmGcz-iR4.original.fullsize.png)

æ­¤å¤–ï¼Œæ³¨æ„optimum model complexityçš„ä½ç½®å¹¶ä¸æ˜¯biaså’Œvarianceäº¤å‰çš„ç‚¹ï¼è¿™æ˜¯å¾ˆå¤šæ•™ç§‘ä¹¦ç”»çš„å›¾æœ‰é—®é¢˜çš„åœ°æ–¹... å› ä¸ºoptimalå…¶å®æ˜¯total errorçš„æœ€ä½ç‚¹ï¼Œbiaså’Œvarianceå¯èƒ½åœ¨ä»»ä½•ä½ç½®ï¼Œå°±åƒè¿™å¼ å›¾ã€‚

è¿™é‡Œçš„total erroræŒ‡çš„æ˜¯testing error æˆ–è€…è¯´validation errorã€‚å‡å¦‚æˆ‘ä»¬æŠŠæ•°æ®åˆ†æˆä¸‰ç±»ï¼Œtraining, validation å’Œholdoutï¼Œé‚£ä¹ˆè¿™ä¸ªerrorå°±æ˜¯validationï¼›å¦‚æœåˆ†æˆä¸‰ç±»ï¼Œtrainingå’Œtestingï¼Œé‚£ä¹ˆè¿™ä¸ªerrorå°±æ˜¯testing errorã€‚

![](https://cdn.mathpix.com/snip/images/9ftC6__Z5eJi1W2vu_MW9oioQDImo_qZBS79MWNHBOw.original.fullsize.png)

å³ä¸Šè§’, high variance, low bias;

å·¦ä¸‹è§’, low variance, high bias;

å³ä¸‹è§’, high variance, high bias.

å›¾ä¸­çš„è“ç‚¹ä»£è¡¨åœ¨xæ˜¯åŒä¸€ä¸ªå€¼çš„æ—¶å€™ç¨ç¨å˜æ¢æ¨¡å‹åï¼ˆå‚æ•°ç»†å¾®å˜åŒ–ï¼‰å¾—åˆ°çš„é¢„æµ‹ç»“æœï¼Œå¹¶ä¸æ˜¯å¤šä¸ªä¸åŒçš„xå€¼ï¼

### è§£å†³overfittingçš„é—®é¢˜

1. æé«˜æ ·æœ¬é‡
2. è§£å†³æ¨¡å‹è¿‡äºå¤æ‚çš„é—®é¢˜  - filter out features å‡å°‘featureçš„ä¸ªæ•°ï¼ˆeg PCAï¼‰ - regularization æ­£åˆ™åŒ–ï¼ˆRidgeï¼ŒLasso\) ä½¿æ¨¡å‹çš„ç¨³å®šæ€§æé«˜

å¦å¤–overfittingæ˜¯ä¸€ä¸ªç›¸å¯¹çš„æ¦‚å¿µï¼Œå¦‚æœæ‰‹å¤´åªæœ‰ä¸€ä¸ªmodelï¼Œå†å·®ä¹Ÿå¾—ç”¨ã€‚

é¢è¯•é¢˜ï¼šå¦‚æœæˆ‘ä»¬æŠŠtraining dataçš„æ•°é‡å¢åŠ äº†ï¼Œå‘ç°validation errorå‡å°ï¼Œè¯´æ˜overfittedã€‚å› ä¸ºåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­biasæ˜¯ä¸å˜çš„ï¼Œæ¨¡å‹çš„å¤æ‚åº¦ä¸€æ¨¡ä¸€æ ·ï¼Œæˆ‘ä»¬é€šè¿‡åŠ æ•°æ®çš„æ–¹å¼ç«Ÿç„¶èƒ½æ˜¾è‘—å‡å°errorï¼Œé‚£åŸæ¥ä¸€å®šæ˜¯overfittedã€‚è¿™åªæ˜¯ä¸ªè®¤ä¸ºè®¾è®¡çš„é¢è¯•é¢˜ï¼Œå·¥ä½œä¸­æ²¡äººè¿™ä¹ˆå¹²ğŸ¤·â€â™€ï¸ï¼Œæ²¡äº‹å¹²ä¸ºå•¥è¦ç•™ä¸€éƒ¨åˆ†æ•°æ®ä¸“é—¨çœ‹æ˜¯ä¸æ˜¯overfitting...

## Regularization æ­£åˆ™åŒ–

$$
\text {Training Error}=\sum_{i=1}^{n}\left(y_{i}-f\left(\mathrm{x}_{i}\right)\right)^{2}+\text { something }
$$

$$
\text {Loss Function}=\sum_{i=1}^{n}\left(y_{i}-f\left(\mathrm{x}_{i}\right)\right)^{2}, \text { subject to something}
$$

ä¸Š2å¼åº”è¯¥ç­‰ä»·ï¼Œæ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•

æœºå™¨å­¦ä¹ çš„ç›®çš„æ˜¯è®©validation errorå°ï¼Œä½†æ˜¯æˆ‘ä»¬åœ¨è®­ç»ƒæ¨¡å‹çš„æ—¶å€™çš„erroræ˜¯training errorï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨ï¼ˆä¸ç®¡æ˜¯minimum absolute valueè¿˜æ˜¯minimum squareï¼‰è®©training errorå°ï¼Œè¿™ä¸¤ä¸ªerrorä¹‹é—´å¾ˆæ˜æ˜¾æœ‰ä¸€ä¸ªgapã€‚somethingå°±æ˜¯æ¨¡å‹åœ¨validationå’Œtestingä¸­è¡¨ç°çš„å·®å¼‚ã€‚

æƒ©ç½šé¡¹ï¼Œæƒ©ç½šæƒ³ä¼˜åŒ–least squareçš„è¡Œä¸ºï¼Œå…¶å®æ˜¯æŠŠæ‰€æœ‰xçš„ç³»æ•°æ‹¿å‡ºæ¥æƒ©ç½šã€‚æƒ©ç½šé¡¹æ˜¯ç”±lambaï¼ˆä¸€ä¸ªhyperparameterï¼‰çš„å¤§å°æ¥è¡¡é‡ã€‚lambdaæ˜¯è¯•å‡ºæ¥çš„ï¼Œåˆ©ç”¨å«æœ‰labelçš„training dataå»åšcross validationï¼Œåˆ¤æ–­æœ€å¥½çš„lambdaå€¼ã€‚

### Lasso: L1 regularization

Linear:          $$\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|$$ 

Logistic:       $$\operatorname{argmin}_{\beta} \sum_{i=1}^{n}\left[-y_{i} \log \left(h_{\beta}\left(x_{i}\right)\right)-\left(1-y_{i}\right) \log \left(1-h_{\beta}\left(x_{i}\right)\right)\right]+\lambda\|\beta\|_{1}$$ 

### Ridge: L2 regularization

 Linear:            $$\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}+\lambda \sum_{j=1}^{p} \beta_{j}^{2}$$ 

Logistic:       $$\operatorname{argmin}_{\beta} \sum_{i=1}^{n}\left[-y_{i} \log \left(h_{\beta}\left(x_{i}\right)\right)-\left(1-y_{i}\right) \log \left(1-h_{\beta}\left(x_{i}\right)\right)\right]+\lambda\|\beta\|_{2}^{2}$$ 



åªè¦æ˜¯åŸºäºè·ç¦»å®šä¹‰çš„loss functionéƒ½å¯ä»¥åŠ regularizationï¼Œè¿™å’Œæ˜¯å¦æ˜¯generalized linear modelæ— å…³ã€‚



> é¢è¯•è€ƒç‚¹ L1å’ŒL2æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
>
> å¦‚æœå·²ç»é—®åˆ°è¿™é‡Œäº†ï¼Œå…¶å®ä¸åªæ˜¯åœ¨é—®overfittingçš„è§’åº¦ä¸Šregularizationçš„åŒºåˆ«ï¼Œå› ä¸ºå®ƒä»¬è§£å†³overfittingçš„èƒ½åŠ›æ˜¯ä¸€æ ·çš„ï¼Œæ²¡æœ‰åŒºåˆ«ã€‚è¿™é‡Œå…¶å®åœ¨é—®feature selectionï¼ŒL2å¯¹äºcorrelatedçš„featureåŒç­‰å¯¹å¾…ï¼Œç»™å‡ºçš„ç³»æ•°éƒ½ä¸€æ ·ï¼Œè¿™æ ·æ›´stableï¼›è€ŒL1å¯ä»¥ç»™å‡ºfeatureã€‚å¹³æ–¹é¡¹ç»™çš„penaltyå¾ˆå¤§ï¼Œå€¾å‘äºè®©æ‰€æœ‰featureç£¨å¹³ï¼Œè€Œç»å¯¹å€¼å…¶å®è®©æŸäº›featureä¿ç•™ï¼ŒæŸäº›ç³»æ•°å˜æˆäº†0ã€‚æ­¤å¤–ä¹Ÿå¯ä»¥ç”¨random forestçš„feature importanceåšfeature selectionã€‚ ä½†æ˜¯è¿™ä¸¤ç§è®¤ä¸ºçš„feature importanceå…¶å®ä¸å¤ªä¸€æ ·ï¼Œå› ä¸ºRFæ˜¯non-linear modelã€‚



## å·¥ä½œä¸­å¦‚ä½•è§£å†³feature selection

1. PCA ä»100ä¸ªfeatureç•™ä¸‹3ä¸ªï¼Œä½†æ˜¯è¿™3ä¸ªfeatureçš„ç‰©ç†æ„ä¹‰å·²ç»ä¸çŸ¥é“äº†ï¼Œæ‰€ä»¥ä¸€èˆ¬ä¸ç”¨ã€‚ 
2. ä¸¢feature å…ˆåšfeature importanceï¼ŒæŠŠä¸å¤ªé‡è¦çš„æ‰”äº†ï¼Œä¸ºäº†model performanceï¼Œæé«˜é€Ÿåº¦ 
3. regularization 

